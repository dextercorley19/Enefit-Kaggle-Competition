{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import enefit\n",
    "import holidays\n",
    "import datetime\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from lightgbm import early_stopping\n",
    "from lightgbm import log_evaluation\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../Enefit_Datasets/train.csv')\n",
    "gas_df = pd.read_csv('../Enefit_Datasets/gas_prices.csv')\n",
    "electricity_df = pd.read_csv('../Enefit_Datasets/electricity_prices.csv')\n",
    "client_df = pd.read_csv('../Enefit_Datasets/client.csv')\n",
    "fw_df = pd.read_csv('../Enefit_Datasets/forecast_weather.csv')\n",
    "hw_df = pd.read_csv('../Enefit_Datasets/historical_weather.csv')\n",
    "locations = pd.read_csv('../Enefit_Datasets/county_lon_lats.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TrainPreprocessing(train, client_df, fw_df, electricity_df, gas_df, locations, hw_df):\n",
    "    '''Train data preprocessing'''\n",
    "    \n",
    "    # Drop nan values\n",
    "    train = train[train['target'].notnull()]\n",
    "    \n",
    "    # Converting to datetime & date feature engineering\n",
    "    train['datetime'] = pd.to_datetime(train['datetime'], utc=True)\n",
    "    \n",
    "    train['year'] = train['datetime'].dt.year\n",
    "    train['quarter'] = train['datetime'].dt.quarter\n",
    "    train['month'] = train['datetime'].dt.month\n",
    "    train['week'] = train['datetime'].dt.isocalendar().week\n",
    "    train['day'] = train['datetime'].dt.day\n",
    "    train['hour'] = train['datetime'].dt.hour\n",
    "    train['dayofweek'] = train['datetime'].dt.dayofweek\n",
    "    train['dayofyear'] = train['datetime'].dt.dayofyear\n",
    "\n",
    "    train['week'] = train['week'].astype('int32')\n",
    "    \n",
    "    train[\"jan_flag\"] = (train[\"month\"] == 1).astype(int)\n",
    "\n",
    "    # Client data processing\n",
    "    # Subtracting 2 from data_block_id. Data is two steps ahead\n",
    "    client_df['data_block_id'] -= 2\n",
    "    \n",
    "    # Average installed capacity. (installed_capacity / eic_count)\n",
    "    client_df['avg_installed_cap'] = client_df['installed_capacity'] / client_df['eic_count']\n",
    "\n",
    "    \n",
    "    # Electricity data processing\n",
    "    # Renaming (forecast_date) to (datetime) for merging with the train data later\n",
    "    electricity_df = electricity_df.rename(columns= {'forecast_date' : 'datetime'})\n",
    "    \n",
    "    # Converting (datetime) column to datetime\n",
    "    electricity_df['datetime'] = pd.to_datetime(electricity_df['datetime'], utc= True)\n",
    "    \n",
    "    electricity_df['hour'] = electricity_df['datetime'].dt.hour\n",
    "    \n",
    "    # Locations data processing\n",
    "    # Drop\n",
    "    locations = locations.drop('Unnamed: 0', axis= 1) \n",
    "        \n",
    "\n",
    "    # Forecast Weather \n",
    "    # 1. Rounding lat & lon to 1 decimal place\n",
    "    # 2. Merge counties data from locations DF to lat & lon in forecast weather df\n",
    "    # 3. Drop null rows\n",
    "    # 4. Convert county col to int dtype\n",
    "    # 5. Drop un-necessary columns\n",
    "    # 6. Rename date column to 'datetime' and convert to datetime dtype\n",
    "    # 7. New df with mean weather values per hour. Convert datetime back to normal datetime format in new df.\n",
    "    # 8. New df with mean values per hour grouped also by county. Convert datetime col back to datetime.\n",
    "    \n",
    "    # 1.\n",
    "    fw_df[['latitude', 'longitude']] = fw_df[['latitude', 'longitude']].astype(float).round(1)\n",
    "    \n",
    "    # 2.\n",
    "    fw_df = fw_df.merge(locations, how='left', on=['latitude', 'longitude'])\n",
    "    \n",
    "    # 3.\n",
    "    fw_df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # 4.\n",
    "    fw_df['county'] = fw_df['county'].astype('int64')\n",
    "    \n",
    "    # 5.\n",
    "    fw_df.drop(['origin_datetime', 'latitude', 'longitude', 'hours_ahead',\n",
    "               'data_block_id'], axis=1, inplace=True)\n",
    "    \n",
    "    # 6.\n",
    "    fw_df.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n",
    "    fw_df['datetime'] = pd.to_datetime(fw_df['datetime'], utc=True)\n",
    "    \n",
    "    # 7.\n",
    "    fw_df_mean = fw_df.groupby([fw_df['datetime']\n",
    "                                .dt.to_period('h')])[list(fw_df.drop(['county', 'datetime'], axis=1)\n",
    "                                                                           .columns)].mean().reset_index()\n",
    "    fw_df_mean['datetime'] = pd.to_datetime(fw_df_mean['datetime'].dt.to_timestamp(), utc=True)\n",
    "    \n",
    "    # 8. \n",
    "    fw_df_county = fw_df.groupby(['county', fw_df['datetime'].dt.to_period('h')])[list(fw_df.drop(['county', 'datetime'], axis=1).columns)].mean().reset_index()\n",
    "    fw_df_county['datetime'] = pd.to_datetime(fw_df_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "\n",
    "    \n",
    "    # Historical weather df processing\n",
    "    # 1. Rounding lat & lon to 1 decimal place\n",
    "    # 2. Merge counties data from locations DF to lat & lon in forecast weather df\n",
    "    # 3. Drop null rows\n",
    "    # 4. Convert county col to int dtype\n",
    "    # 5. Drop un-necessary columns\n",
    "    # 6. Rename date column to 'datetime' and convert to datetime dtype\n",
    "    # 7. New df with mean weather values per hour. Convert datetime back to normal datetime format in new df.\n",
    "    # 8. New df with mean values per hour grouped also by county. Convert datetime col back to datetime.\n",
    "    # 9. Merge data_block_id back to new county df\n",
    "   \n",
    "    # 1.\n",
    "    hw_df[['latitude', 'longitude']] = hw_df[['latitude', 'longitude']].astype(float).round(1)\n",
    "    \n",
    "    # 2.\n",
    "    hw_df = hw_df.merge(locations, how='left', on=['longitude', 'latitude'])\n",
    "    \n",
    "    # 3.\n",
    "    hw_df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # 4.\n",
    "    hw_df['county'] = hw_df['county'].astype('int64')\n",
    "    \n",
    "    # 5.\n",
    "    hw_df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "    \n",
    "    # 6.\n",
    "    hw_df['datetime'] = pd.to_datetime(hw_df['datetime'], utc=True)\n",
    "    \n",
    "    # 7.\n",
    "    hw_df_mean = hw_df.groupby([hw_df['datetime']\n",
    "                                .dt.to_period('h')])[list(hw_df.drop(['county', 'datetime', 'data_block_id'], axis=1)\n",
    "                                                                           .columns)].mean().reset_index()\n",
    "    hw_df_mean['datetime'] = pd.to_datetime(hw_df_mean['datetime'].dt.to_timestamp(), utc=True)\n",
    "    \n",
    "    hw_df_mean = hw_df_mean.merge(hw_df[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "    \n",
    "    # 8. \n",
    "    hw_df_county = hw_df.groupby(['county', hw_df['datetime'].dt.to_period('h')])[list(hw_df.drop(['county', 'datetime', 'data_block_id'], axis=1).columns)].mean().reset_index()\n",
    "    hw_df_county['datetime'] = pd.to_datetime(hw_df_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "    hw_df_county = hw_df_county.merge(hw_df[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "    hw_df_county.drop_duplicates(inplace=True)\n",
    "    hw_df_county.reset_index()\n",
    "    \n",
    "    # Merge the data into train set\n",
    "    # 1. Merge client \n",
    "    # 2. Merge gas\n",
    "    # 3. Merge electricity\n",
    "    # 4. Merge forecast weather\n",
    "    # 5. Merge forecast weather by county\n",
    "    # 6. Add hour col to hist weather and hist county. Drop dups and datetime col\n",
    "    # 7. Merge hist weather menas\n",
    "    # 8. Merge hist weather means by county\n",
    "    # 9. Fill null values with forward and backward method\n",
    "    # 10. Drop un-necessary cols...?\n",
    "\n",
    "    \n",
    "    # 1\n",
    "    train = train.merge(client_df.drop(columns = ['date']), how='left', on = ['data_block_id', 'county', 'is_business', 'product_type'])\n",
    "    \n",
    "    # 2\n",
    "    train = train.merge(gas_df[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n",
    "    \n",
    "    # 3\n",
    "    train = train.merge(electricity_df[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour','data_block_id'])\n",
    "    \n",
    "    # 4 \n",
    "    train = train.merge(fw_df_mean, how='left', on='datetime')\n",
    "    \n",
    "    # 5 \n",
    "    train = train.merge(fw_df_county, how='left', on=['datetime', 'county'], suffixes = ('_fcast_mean', '_fcast_mean_by_county'))\n",
    "    \n",
    "    # 6\n",
    "    hw_df_mean['hour'] = hw_df_mean['datetime'].dt.hour\n",
    "    hw_df_county['hour'] = hw_df_county['datetime'].dt.hour\n",
    "    \n",
    "    hw_df_mean.drop_duplicates(inplace=True)\n",
    "    hw_df_county.drop_duplicates(inplace=True)\n",
    "    hw_df_mean.drop('datetime', axis=1, inplace=True)\n",
    "    hw_df_county.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "    # 7\n",
    "    train = train.merge(hw_df_mean, how='left', on=['data_block_id', 'hour'])\n",
    "    \n",
    "    # 8 \n",
    "    train = train.merge(hw_df_county, how='left', on=['data_block_id', 'county', 'hour'], suffixes= ('_hist_mean', '_hist_mean_by_county'))\n",
    "    \n",
    "    # 9\n",
    "    train = train.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n",
    "    \n",
    "    # 10\n",
    "    train.drop(['row_id', 'data_block_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_holiday(dataframe):\n",
    "    '''Creates is_holiday boolean column for Estonian country wide holidays'''\n",
    "    estonian_holidays = holidays.country_holidays('EE', years=range(2021, 2024))\n",
    "    estonian_holidays = list(estonian_holidays.keys())\n",
    "\n",
    "    \n",
    "    dataframe['is_holiday'] = dataframe.apply(lambda row: (datetime.date(row['year'], row['month'], row['day']) in estonian_holidays) * 1, axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_target_lags(train, N_lags):\n",
    "    '''Lag target variables are a form of feature engineering that gives\n",
    "    the model context for what the target was N days ago'''\n",
    "    \n",
    "    # Store datetime column in a python object\n",
    "    og_datetime = train['datetime']\n",
    "    \n",
    "    # Store important variables to reference with target\n",
    "    target_and_friends = train[['datetime', 'prediction_unit_id', 'is_consumption', 'target']].copy()\n",
    "    \n",
    "    # Create lagged targets\n",
    "    for lag in range(2, N_lags + 1): # Starts at 2, ends at set day\n",
    "        target_and_friends['datetime'] = og_datetime + pd.DateOffset(lag) # Shifts dates forward by set amount (lag)\n",
    "        train = train.merge(target_and_friends, how='left', on=['datetime', 'prediction_unit_id', 'is_consumption'], suffixes = ('', f'_{lag}_days_ago'))\n",
    "        \n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train preprocessing\n",
    "train = TrainPreprocessing(train, client_df, fw_df, electricity_df, gas_df, locations, hw_df)\n",
    "\n",
    "# Add holidays\n",
    "train = is_holiday(train)\n",
    "\n",
    "# Add target lags\n",
    "N_lags = 7\n",
    "train = create_target_lags(train, N_lags) #Any number after 2\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop year column as its no longer needed\n",
    "train.drop(['year'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Further feature engineering\n",
    "\n",
    "# Convert datetime to integer for date features\n",
    "train['datetime'] = train['datetime'].astype('int64')\n",
    "\n",
    "train['sin_hour']= (np.pi * np.sin(train['hour']) / 12)\n",
    "train['cos_hour']= (np.pi * np.cos(train['hour']) / 12)\n",
    "train['sin_dayofyear']= (np.pi * np.sin(train['dayofyear']) / 183)\n",
    "train['cos_dayofyear']= (np.pi * np.cos(train['dayofyear']) / 183)\n",
    "train['target_mean']= train[[f'target_{i}_days_ago' for i in range(2, N_lags+1)]].mean(1)\n",
    "train['target_std']= train[[f'target_{i}_days_ago' for i in range(2, N_lags+1)]].std(1)\n",
    "train['target_var']= train[[f'target_{i}_days_ago' for i in range(2, N_lags+1)]].var(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Log columns with outliers\n",
    "to_log= ['installed_capacity', 'euros_per_mwh', 'temperature_fcast_mean', 'dewpoint_fcast_mean',\n",
    "        'cloudcover_high_fcast_mean', 'cloudcover_low_fcast_mean', 'cloudcover_mid_fcast_mean', 'cloudcover_total_fcast_mean',\n",
    "        '10_metre_u_wind_component_fcast_mean', '10_metre_v_wind_component_fcast_mean', 'direct_solar_radiation_fcast_mean',\n",
    "        'snowfall_fcast_mean', 'total_precipitation_fcast_mean', 'temperature_fcast_mean_by_county', 'dewpoint_fcast_mean_by_county',\n",
    "        'cloudcover_high_fcast_mean_by_county', 'cloudcover_low_fcast_mean_by_county', 'cloudcover_mid_fcast_mean_by_county',\n",
    "        'cloudcover_total_fcast_mean_by_county', '10_metre_u_wind_component_fcast_mean_by_county', '10_metre_v_wind_component_fcast_mean_by_county',\n",
    "        'surface_solar_radiation_downwards_fcast_mean_by_county', 'snowfall_fcast_mean_by_county', 'total_precipitation_fcast_mean_by_county',\n",
    "        'rain_hist_mean', 'snowfall_hist_mean', 'windspeed_10m_hist_mean_by_county', 'target_2_days_ago', 'target_3_days_ago',\n",
    "        'target_4_days_ago', 'target_5_days_ago', 'target_6_days_ago', 'target_7_days_ago', 'target_mean', 'target_std']\n",
    "for i in to_log:\n",
    "    train[f\"log_{i}\"]= np.where((train[i])!= 0, np.log(train[i]),0) #Create new col and log the value if not 0\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the display options to show all rows and columns\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Your code here\n",
    "train_viz = train.groupby(['hour', 'county'])\n",
    "train_viz_df = pd.DataFrame(train_viz)\n",
    "\n",
    "train_viz.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestPreprocessing(test, client_df, fw_df, electricity_df, gas_df, locations, hw_df):\n",
    "    '''Test data preprocessing'''\n",
    "\n",
    "    # Converting to datetime & date feature engineering\n",
    "    test = test.rename(columns={'prediction_datetime' : 'datetime'})\n",
    "\n",
    "    test['datetime'] = pd.to_datetime(test['datetime'], utc=True)\n",
    "\n",
    "    test['year'] = test['datetime'].dt.year\n",
    "    test['quarter'] = test['datetime'].dt.quarter\n",
    "    test['month'] = test['datetime'].dt.month\n",
    "    test['week'] = test['datetime'].dt.isocalendar().week\n",
    "    test['day'] = test['datetime'].dt.day\n",
    "    test['hour'] = test['datetime'].dt.hour\n",
    "    test['dayofweek'] = test['datetime'].dt.dayofweek\n",
    "    test['dayofyear'] = test['datetime'].dt.dayofyear\n",
    "\n",
    "    test['week'] = test['week'].astype('int64')\n",
    "\n",
    "    test[\"jan_flag\"] = (test[\"month\"] == 1).astype(int)\n",
    "\n",
    "    # Client data processing\n",
    "    # Subtracting 2 from data_block_id. Data is two steps ahead\n",
    "    client_df['data_block_id'] -= 2\n",
    "    \n",
    "    # Average installed capacity. (installed_capacity / eic_count)\n",
    "    client_df['avg_installed_cap'] = client_df['installed_capacity'] / client_df['eic_count']\n",
    "\n",
    "    \n",
    "    # Electricity data processing\n",
    "    # Renaming (forecast_date) to (datetime) for merging with the test data later\n",
    "    electricity_df = electricity_df.rename(columns= {'forecast_date' : 'datetime'})\n",
    "    \n",
    "    # Converting (datetime) column to datetime\n",
    "    electricity_df['datetime'] = pd.to_datetime(electricity_df['datetime'], utc= True)\n",
    "    \n",
    "    electricity_df['hour'] = electricity_df['datetime'].dt.hour\n",
    "    \n",
    "    # Locations data processing\n",
    "    # Drop\n",
    "    locations = locations.drop('Unnamed: 0', axis= 1) \n",
    "        \n",
    "\n",
    "    # Forecast Weather \n",
    "    # 1. Rounding lat & lon to 1 decimal place\n",
    "    # 2. Merge counties data from locations DF to lat & lon in forecast weather df\n",
    "    # 3. Drop null rows\n",
    "    # 4. Convert county col to int dtype\n",
    "    # 5. Drop un-necessary columns\n",
    "    # 6. Rename date column to 'datetime' and convert to datetime dtype\n",
    "    # 7. New df with mean weather values per hour. Convert datetime back to normal datetime format in new df.\n",
    "    # 8. New df with mean values per hour grouped also by county. Convert datetime col back to datetime.\n",
    "    \n",
    "    # 1.\n",
    "    fw_df[['latitude', 'longitude']] = fw_df[['latitude', 'longitude']].astype(float).round(1)\n",
    "    \n",
    "    # 2.\n",
    "    fw_df = fw_df.merge(locations, how='left', on=['latitude', 'longitude'])\n",
    "    \n",
    "    # 3.\n",
    "    fw_df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # 4.\n",
    "    fw_df['county'] = fw_df['county'].astype('int64')\n",
    "    \n",
    "    # 5.\n",
    "    fw_df.drop(['origin_datetime', 'latitude', 'longitude', 'hours_ahead',\n",
    "               'data_block_id'], axis=1, inplace=True)\n",
    "    \n",
    "    # 6.\n",
    "    fw_df.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n",
    "    fw_df['datetime'] = pd.to_datetime(fw_df['datetime'], utc=True)\n",
    "    \n",
    "    # 7.\n",
    "    fw_df_mean = fw_df.groupby([fw_df['datetime']\n",
    "                                .dt.to_period('h')])[list(fw_df.drop(['county', 'datetime'], axis=1)\n",
    "                                                                           .columns)].mean().reset_index()\n",
    "    fw_df_mean['datetime'] = pd.to_datetime(fw_df_mean['datetime'].dt.to_timestamp(), utc=True)\n",
    "    \n",
    "    # 8. \n",
    "    fw_df_county = fw_df.groupby(['county', fw_df['datetime'].dt.to_period('h')])[list(fw_df.drop(['county', 'datetime'], axis=1).columns)].mean().reset_index()\n",
    "    fw_df_county['datetime'] = pd.to_datetime(fw_df_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "\n",
    "    \n",
    "    # Historical weather df processing\n",
    "    # 1. Rounding lat & lon to 1 decimal place\n",
    "    # 2. Merge counties data from locations DF to lat & lon in forecast weather df\n",
    "    # 3. Drop null rows\n",
    "    # 4. Convert county col to int dtype\n",
    "    # 5. Drop un-necessary columns\n",
    "    # 6. Rename date column to 'datetime' and convert to datetime dtype\n",
    "    # 7. New df with mean weather values per hour. Convert datetime back to normal datetime format in new df.\n",
    "    # 8. New df with mean values per hour grouped also by county. Convert datetime col back to datetime.\n",
    "    # 9. Merge data_block_id back to new county df\n",
    "   \n",
    "    # 1.\n",
    "    hw_df[['latitude', 'longitude']] = hw_df[['latitude', 'longitude']].astype(float).round(1)\n",
    "    \n",
    "    # 2.\n",
    "    hw_df = hw_df.merge(locations, how='left', on=['longitude', 'latitude'])\n",
    "    \n",
    "    # 3.\n",
    "    hw_df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # 4.\n",
    "    hw_df['county'] = hw_df['county'].astype('int64')\n",
    "    \n",
    "    # 5.\n",
    "    hw_df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "    \n",
    "    # 6.\n",
    "    hw_df['datetime'] = pd.to_datetime(hw_df['datetime'], utc=True)\n",
    "    \n",
    "    # 7.\n",
    "    hw_df_mean = hw_df.groupby([hw_df['datetime']\n",
    "                                .dt.to_period('h')])[list(hw_df.drop(['county', 'datetime', 'data_block_id'], axis=1)\n",
    "                                                                           .columns)].mean().reset_index()\n",
    "    hw_df_mean['datetime'] = pd.to_datetime(hw_df_mean['datetime'].dt.to_timestamp(), utc=True)\n",
    "    \n",
    "    hw_df_mean = hw_df_mean.merge(hw_df[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "    \n",
    "    # 8. \n",
    "    hw_df_county = hw_df.groupby(['county', hw_df['datetime'].dt.to_period('h')])[list(hw_df.drop(['county', 'datetime', 'data_block_id'], axis=1).columns)].mean().reset_index()\n",
    "    hw_df_county['datetime'] = pd.to_datetime(hw_df_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "    hw_df_county = hw_df_county.merge(hw_df[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "    hw_df_county.drop_duplicates(inplace=True)\n",
    "    hw_df_county.reset_index()\n",
    "    \n",
    "    # Merge the data into test set\n",
    "    # 1. Merge client \n",
    "    # 2. Merge gas\n",
    "    # 3. Merge electricity\n",
    "    # 4. Merge forecast weather\n",
    "    # 5. Merge forecast weather by county\n",
    "    # 6. Add hour col to hist weather and hist county. Drop dups and datetime col\n",
    "    # 7. Merge hist weather menas\n",
    "    # 8. Merge hist weather means by county\n",
    "    # 9. Fill null values with forward and backward method\n",
    "    # 10. Drop un-necessary cols...?\n",
    "\n",
    "    \n",
    "    # 1\n",
    "    test = test.merge(client_df.drop(columns = ['date']), how='left', on = ['data_block_id', 'county', 'is_business', 'product_type'])\n",
    "    \n",
    "    # 2\n",
    "    test = test.merge(gas_df[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n",
    "    \n",
    "    # 3\n",
    "    test = test.merge(electricity_df[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour','data_block_id'])\n",
    "    \n",
    "    # 4 \n",
    "    test = test.merge(fw_df_mean, how='left', on='datetime')\n",
    "    \n",
    "    # 5 \n",
    "    test = test.merge(fw_df_county, how='left', on=['datetime', 'county'], suffixes = ('_fcast_mean', '_fcast_mean_by_county'))\n",
    "    \n",
    "    # 6\n",
    "    hw_df_mean['hour'] = hw_df_mean['datetime'].dt.hour\n",
    "    hw_df_county['hour'] = hw_df_county['datetime'].dt.hour\n",
    "    \n",
    "    hw_df_mean.drop_duplicates(inplace=True)\n",
    "    hw_df_county.drop_duplicates(inplace=True)\n",
    "    hw_df_mean.drop('datetime', axis=1, inplace=True)\n",
    "    hw_df_county.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "    # 7\n",
    "    test = test.merge(hw_df_mean, how='left', on=['data_block_id', 'hour'])\n",
    "    \n",
    "    # 8 \n",
    "    test = test.merge(hw_df_county, how='left', on=['data_block_id', 'county', 'hour'], suffixes= ('_hist_mean', '_hist_mean_by_county'))\n",
    "    \n",
    "    # 9\n",
    "    test = test.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n",
    "    \n",
    "    # 10\n",
    "    test.drop(['row_id', 'data_block_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_revealed_targets_test(data, previous_revealed_targets, N_lags):\n",
    "    '''🎯 Create new test data based on previous_revealed_targets and N_day_lags 🎯 ''' \n",
    "    for count, revealed_targets in enumerate(previous_revealed_targets) :\n",
    "        day_lag = count + 2\n",
    "        \n",
    "        # Get hour\n",
    "        revealed_targets['hour'] = pd.to_datetime(revealed_targets['datetime'], utc= True).dt.hour\n",
    "        \n",
    "        # Select columns and rename target\n",
    "        revealed_targets = revealed_targets[['hour', 'prediction_unit_id', 'is_consumption', 'target']]\n",
    "        revealed_targets = revealed_targets.rename(columns = {\"target\" : f\"target_{day_lag}_days_ago\"})\n",
    "        \n",
    "        \n",
    "        # Add past revealed targets\n",
    "        data = pd.merge(data,\n",
    "                        revealed_targets,\n",
    "                        how = 'left',\n",
    "                        on = ['hour', 'prediction_unit_id', 'is_consumption'],\n",
    "                       )\n",
    "        \n",
    "    # If revealed_target_columns not available, replace by nan\n",
    "    all_revealed_columns = [f\"target_{day_lag}_days_ago\" for day_lag in range(2, N_lags+1)]\n",
    "    missing_columns = list(set(all_revealed_columns) - set(data.columns))\n",
    "    data[missing_columns] = np.nan \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = enefit.make_env()\n",
    "iter_test = env.iter_test()\n",
    "previous_revealed_targets = []\n",
    "for (test, revealed_targets, client_test, historical_weather_test,\n",
    "     forecast_weather_test, electricity_test, gas_test,sample_prediction) in iter_test:\n",
    "    \n",
    "    # Initiate column data_block_id with default value to merge the data on\n",
    "    id_column = 'data_block_id' \n",
    "    \n",
    "    test[id_column] = 0\n",
    "    gas_test[id_column] = 0\n",
    "    electricity_test[id_column] = 0\n",
    "    historical_weather_test[id_column] = 0\n",
    "    forecast_weather_test[id_column] = 0\n",
    "    client_test[id_column] = 0\n",
    "    revealed_targets[id_column] = 0\n",
    "\n",
    "    data_test = TestPreprocessing(test, client_test, forecast_weather_test, electricity_test, gas_test, locations, historical_weather_test)\n",
    "    data_test = is_holiday(data_test)\n",
    "    \n",
    "    # Drop year column as its no longer needed\n",
    "    data_test.drop(['year'], axis=1, inplace=True)\n",
    "    data_test['datetime']= pd.to_datetime(data_test['datetime'], utc= True).astype('int64')\n",
    "    \n",
    "    # Store revealed_targets\n",
    "    previous_revealed_targets.insert(0, revealed_targets)\n",
    "    if len(previous_revealed_targets) == N_lags:\n",
    "        previous_revealed_targets.pop()\n",
    "    \n",
    "    # Add previous revealed targets\n",
    "    df_test = create_revealed_targets_test(data = data_test.copy(),\n",
    "                                           previous_revealed_targets = previous_revealed_targets.copy(),\n",
    "                                           N_lags = N_lags\n",
    "                                          )\n",
    "    #Data Transformation\n",
    "    df_test['sin_hour']= (np.pi * np.sin(df_test['hour']) / 12)\n",
    "    df_test['cos_hour']= (np.pi * np.cos(df_test['hour']) / 12)\n",
    "    df_test['sin_dayofyear']= (np.pi * np.sin(df_test['dayofyear']) / 183)\n",
    "    df_test['cos_dayofyear']= (np.pi * np.cos(df_test['dayofyear']) / 183)\n",
    "    df_test['target_mean']= df_test[[f'target_{i}_days_ago' for i in range(2, N_lags+1)]].mean(1)\n",
    "    df_test['target_std']= df_test[[f'target_{i}_days_ago' for i in range(2, N_lags+1)]].std(1)\n",
    "    df_test['target_var']= df_test[[f'target_{i}_days_ago' for i in range(2, N_lags+1)]].var(1)\n",
    "    for i in to_log:\n",
    "        df_test[f\"log_{i}\"]= np.where((df_test[i])!= 0, np.log(df_test[i]),0)\n",
    "    X_test = df_test.drop('currently_scored', axis= 1).values\n",
    "\n",
    "    #Predictions\n",
    "    test['target'] = model.predict(X_test).clip(0)\n",
    "    test['target_solar'] = prod_model.predict(X_test).clip(0)\n",
    "    test.loc[test['is_consumption']==0, \"target\"] = test.loc[test['is_consumption']==0, \"target_solar\"]  \n",
    "    sample_prediction[\"target\"] = test['target']\n",
    "    \n",
    "    #Sending predictions to the API\n",
    "    env.predict(sample_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
