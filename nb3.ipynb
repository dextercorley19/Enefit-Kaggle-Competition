{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 43.9331231101866\n"
     ]
    }
   ],
   "source": [
    "def TestPreprocessing(test, client_df, fw_df, electricity_df, gas_df, locations, hw_df):\n",
    "    '''Test data preprocessing'''\n",
    "\n",
    "    # Converting to datetime & date feature engineering\n",
    "    test = test.rename(columns={'prediction_datetime' : 'datetime'})\n",
    "\n",
    "    test['datetime'] = pd.to_datetime(test['datetime'], utc=True)\n",
    "\n",
    "    test['year'] = test['datetime'].dt.year\n",
    "    test['quarter'] = test['datetime'].dt.quarter\n",
    "    test['month'] = test['datetime'].dt.month\n",
    "    test['week'] = test['datetime'].dt.isocalendar().week\n",
    "    test['day'] = test['datetime'].dt.day\n",
    "    test['hour'] = test['datetime'].dt.hour\n",
    "    test['dayofweek'] = test['datetime'].dt.dayofweek\n",
    "    test['dayofyear'] = test['datetime'].dt.dayofyear\n",
    "\n",
    "    \n",
    "    # Client data processing\n",
    "    # Subtracting 2 from data_block_id. Data is two steps ahead\n",
    "    client_df['data_block_id'] -= 2\n",
    "    \n",
    "    # Average installed capacity. (installed_capacity / eic_count)\n",
    "    client_df['avg_installed_cap'] = client_df['installed_capacity'] / client_df['eic_count']\n",
    "\n",
    "    \n",
    "    # Electricity data processing\n",
    "    # Renaming (forecast_date) to (datetime) for merging with the test data later\n",
    "    electricity_df = electricity_df.rename(columns= {'forecast_date' : 'datetime'})\n",
    "    \n",
    "    # Converting (datetime) column to datetime\n",
    "    electricity_df['datetime'] = pd.to_datetime(electricity_df['datetime'], utc= True)\n",
    "    \n",
    "    electricity_df['hour'] = electricity_df['datetime'].dt.hour\n",
    "    \n",
    "    # Locations data processing\n",
    "    # Drop\n",
    "    locations = locations.drop('Unnamed: 0', axis= 1) \n",
    "        \n",
    "\n",
    "    # Forecast Weather \n",
    "    # 1. Rounding lat & lon to 1 decimal place\n",
    "    # 2. Merge counties data from locations DF to lat & lon in forecast weather df\n",
    "    # 3. Drop null rows\n",
    "    # 4. Convert county col to int dtype\n",
    "    # 5. Drop un-necessary columns\n",
    "    # 6. Rename date column to 'datetime' and convert to datetime dtype\n",
    "    # 7. New df with mean weather values per hour. Convert datetime back to normal datetime format in new df.\n",
    "    # 8. New df with mean values per hour grouped also by county. Convert datetime col back to datetime.\n",
    "    \n",
    "    # 1.\n",
    "    fw_df[['latitude', 'longitude']] = fw_df[['latitude', 'longitude']].astype(float).round(1)\n",
    "    \n",
    "    # 2.\n",
    "    fw_df = fw_df.merge(locations, how='left', on=['latitude', 'longitude'])\n",
    "    \n",
    "    # 3.\n",
    "    fw_df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # 4.\n",
    "    fw_df['county'] = fw_df['county'].astype('int64')\n",
    "    \n",
    "    # 5.\n",
    "    fw_df.drop(['origin_datetime', 'latitude', 'longitude', 'hours_ahead',\n",
    "               'data_block_id'], axis=1, inplace=True)\n",
    "    \n",
    "    # 6.\n",
    "    fw_df.rename(columns={'forecast_datetime': 'datetime'}, inplace=True)\n",
    "    fw_df['datetime'] = pd.to_datetime(fw_df['datetime'], utc=True)\n",
    "    \n",
    "    # 7.\n",
    "    fw_df_mean = fw_df.groupby([fw_df['datetime']\n",
    "                                .dt.to_period('h')])[list(fw_df.drop(['county', 'datetime'], axis=1)\n",
    "                                                                           .columns)].mean().reset_index()\n",
    "    fw_df_mean['datetime'] = pd.to_datetime(fw_df_mean['datetime'].dt.to_timestamp(), utc=True)\n",
    "    \n",
    "    # 8. \n",
    "    fw_df_county = fw_df.groupby(['county', fw_df['datetime'].dt.to_period('h')])[list(fw_df.drop(['county', 'datetime'], axis=1).columns)].mean().reset_index()\n",
    "    fw_df_county['datetime'] = pd.to_datetime(fw_df_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "\n",
    "    \n",
    "    # Historical weather df processing\n",
    "    # 1. Rounding lat & lon to 1 decimal place\n",
    "    # 2. Merge counties data from locations DF to lat & lon in forecast weather df\n",
    "    # 3. Drop null rows\n",
    "    # 4. Convert county col to int dtype\n",
    "    # 5. Drop un-necessary columns\n",
    "    # 6. Rename date column to 'datetime' and convert to datetime dtype\n",
    "    # 7. New df with mean weather values per hour. Convert datetime back to normal datetime format in new df.\n",
    "    # 8. New df with mean values per hour grouped also by county. Convert datetime col back to datetime.\n",
    "    # 9. Merge data_block_id back to new county df\n",
    "   \n",
    "    # 1.\n",
    "    hw_df[['latitude', 'longitude']] = hw_df[['latitude', 'longitude']].astype(float).round(1)\n",
    "    \n",
    "    # 2.\n",
    "    hw_df = hw_df.merge(locations, how='left', on=['longitude', 'latitude'])\n",
    "    \n",
    "    # 3.\n",
    "    hw_df.dropna(axis=0, inplace=True)\n",
    "    \n",
    "    # 4.\n",
    "    hw_df['county'] = hw_df['county'].astype('int64')\n",
    "    \n",
    "    # 5.\n",
    "    hw_df.drop(['latitude', 'longitude'], axis=1, inplace=True)\n",
    "    \n",
    "    # 6.\n",
    "    hw_df['datetime'] = pd.to_datetime(hw_df['datetime'], utc=True)\n",
    "    \n",
    "    # 7.\n",
    "    hw_df_mean = hw_df.groupby([hw_df['datetime']\n",
    "                                .dt.to_period('h')])[list(hw_df.drop(['county', 'datetime', 'data_block_id'], axis=1)\n",
    "                                                                           .columns)].mean().reset_index()\n",
    "    hw_df_mean['datetime'] = pd.to_datetime(hw_df_mean['datetime'].dt.to_timestamp(), utc=True)\n",
    "    \n",
    "    hw_df_mean = hw_df_mean.merge(hw_df[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "    \n",
    "    # 8. \n",
    "    hw_df_county = hw_df.groupby(['county', hw_df['datetime'].dt.to_period('h')])[list(hw_df.drop(['county', 'datetime', 'data_block_id'], axis=1).columns)].mean().reset_index()\n",
    "    hw_df_county['datetime'] = pd.to_datetime(hw_df_county['datetime'].dt.to_timestamp(), utc=True)\n",
    "    hw_df_county = hw_df_county.merge(hw_df[['datetime', 'data_block_id']], how='left', on='datetime')\n",
    "    hw_df_county.drop_duplicates(inplace=True)\n",
    "    hw_df_county.reset_index()\n",
    "    \n",
    "    # Merge the data into test set\n",
    "    # 1. Merge client \n",
    "    # 2. Merge gas\n",
    "    # 3. Merge electricity\n",
    "    # 4. Merge forecast weather\n",
    "    # 5. Merge forecast weather by county\n",
    "    # 6. Add hour col to hist weather and hist county. Drop dups and datetime col\n",
    "    # 7. Merge hist weather menas\n",
    "    # 8. Merge hist weather means by county\n",
    "    # 9. Fill null values with forward and backward method\n",
    "    # 10. Drop un-necessary cols...?\n",
    "\n",
    "    \n",
    "    # 1\n",
    "    test = test.merge(client_df.drop(columns = ['date']), how='left', on = ['data_block_id', 'county', 'is_business', 'product_type'])\n",
    "    \n",
    "    # 2\n",
    "    test = test.merge(gas_df[['data_block_id', 'lowest_price_per_mwh', 'highest_price_per_mwh']], how='left', on='data_block_id')\n",
    "    \n",
    "    # 3\n",
    "    test = test.merge(electricity_df[['euros_per_mwh', 'hour', 'data_block_id']], how='left', on=['hour','data_block_id'])\n",
    "    \n",
    "    # 4 \n",
    "    test = test.merge(fw_df_mean, how='left', on='datetime')\n",
    "    \n",
    "    # 5 \n",
    "    test = test.merge(fw_df_county, how='left', on=['datetime', 'county'], suffixes = ('_fcast_mean', '_fcast_mean_by_county'))\n",
    "    \n",
    "    # 6\n",
    "    hw_df_mean['hour'] = hw_df_mean['datetime'].dt.hour\n",
    "    hw_df_county['hour'] = hw_df_county['datetime'].dt.hour\n",
    "    \n",
    "    hw_df_mean.drop_duplicates(inplace=True)\n",
    "    hw_df_county.drop_duplicates(inplace=True)\n",
    "    hw_df_mean.drop('datetime', axis=1, inplace=True)\n",
    "    hw_df_county.drop('datetime', axis=1, inplace=True)\n",
    "\n",
    "    # 7\n",
    "    test = test.merge(hw_df_mean, how='left', on=['data_block_id', 'hour'])\n",
    "    \n",
    "    # 8 \n",
    "    test = test.merge(hw_df_county, how='left', on=['data_block_id', 'county', 'hour'], suffixes= ('_hist_mean', '_hist_mean_by_county'))\n",
    "    \n",
    "    # 9\n",
    "    test = test.groupby(['year', 'day', 'hour'], as_index=False).apply(lambda x: x.ffill().bfill()).reset_index()\n",
    "    \n",
    "    # 10\n",
    "    test.drop(['row_id', 'data_block_id'], axis = 1, inplace = True)\n",
    "    \n",
    "    return test"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
